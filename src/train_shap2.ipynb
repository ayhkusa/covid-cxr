{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py converted to notebook\n",
    "# sudo -E /opt/tljh/user/bin/pip3 install dill\n",
    "# sudo -E /opt/tljh/user/bin/pip3 install scikit-imageF\n",
    "# sudo -E /opt/tljh/user/bin/pip3 install imblearn\n",
    "# sudo -E /opt/tljh/user/bin/pip3 install opencv-python\n",
    "# sudo -E /opt/tljh/user/bin/pip3 install tensorboard\n",
    "\n",
    "11/21\n",
    "# source activate tensorflow2_latest_p37\n",
    "pip3 install tensorflow --upgrade --force-reinstall\n",
    "(tensorflow2_latest_p37) ubuntu@ip-172-31-82-217:~/covid-cxr/src$ pip3 list | grep tensor\n",
    "tensorboard            2.4.0\n",
    "tensorboard-plugin-wit 1.7.0\n",
    "tensorflow             2.3.1\n",
    "tensorflow-estimator   2.3.0\n",
    "\n",
    "ubuntu@ip-172-31-82-217:~$ whereis activate tensorflow2_latest_p37\n",
    "activate: /home/ubuntu/anaconda3/bin/activate\n",
    "export PATH=/opt/tljh/user/bin/:$PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import dill\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.summary as tf_summary\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from math import ceil\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, CategoricalAccuracy, Precision, Recall, AUC\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "from models.models import *\n",
    "from visualization.visualize import *\n",
    "from custom.metrics import F1Score\n",
    "from data.preprocess import remove_text\n",
    "# ---- shap\n",
    "import shap\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2702793900166695211\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12547044112615108578\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12537938978725043842\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14648653952\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 361444406291114821\n",
      "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(histogram, class_multiplier=None):\n",
    "    '''\n",
    "    Computes weights for each class to be applied in the loss function during training.\n",
    "    :param histogram: A list depicting the number of each item in different class\n",
    "    :param class_multiplier: List of values to multiply the calculated class weights by. For further control of class weighting.\n",
    "    :return: A dictionary containing weights for each class\n",
    "    '''\n",
    "    weights = [None] * len(histogram)\n",
    "    for i in range(len(histogram)):\n",
    "        weights[i] = (1.0 / len(histogram)) * sum(histogram) / histogram[i]\n",
    "    class_weight = {i: weights[i] for i in range(len(histogram))}\n",
    "    if class_multiplier is not None:\n",
    "        class_weight = [class_weight[i] * class_multiplier[i] for i in range(len(histogram))]\n",
    "    print(\"Class weights: \", class_weight)\n",
    "    #debug\n",
    "    print(\"Class weights type:\", type(class_weight))\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_minority_oversample(train_set):\n",
    "    '''\n",
    "    Oversample the minority class using the specified algorithm\n",
    "    :param train_set: Training set image file names and labels\n",
    "    :return: A new training set containing oversampled examples\n",
    "    '''\n",
    "    X_train = shap.train_set[[x for x in train_set.columns if x != 'label']].to_numpy()\n",
    "    if X_train.shape[1] == 1:\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "    Y_train = shap.train_set['label'].to_numpy()\n",
    "    sampler = RandomOverSampler(random_state=np.random.randint(0, high=1000))\n",
    "    X_resampled, Y_resampled = sampler.fit_resample(X_train, Y_train)\n",
    "    filenames = X_resampled[:, 1]     # Filename is in second column\n",
    "    label_strs = X_resampled[:, 2]    # Class name is in second column\n",
    "    print(\"Train set shape before oversampling: \", X_train.shape, \" Train set shape after resampling: \", X_resampled.shape)\n",
    "    train_set_resampled = pd.DataFrame({'filename': filenames, 'label': Y_resampled, 'label_str': label_strs})\n",
    "    return train_set_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(cfg, data, callbacks, verbose=1):\n",
    "    '''\n",
    "    Train a and evaluate model on given data.\n",
    "    :param cfg: Project config (from config.yml)\n",
    "    :param data: dict of partitioned dataset\n",
    "    :param callbacks: list of callbacks for Keras model\n",
    "    :param verbose: Verbosity mode to pass to model.fit_generator()\n",
    "    :return: Trained model and associated performance metrics on the test set\n",
    "    '''\n",
    "\n",
    "    # If set in config file, oversample the minority class\n",
    "    if cfg['TRAIN']['IMB_STRATEGY'] == 'random_oversample':\n",
    "        data['TRAIN'] = random_minority_oversample(data['TRAIN'])\n",
    "\n",
    "    # Create ImageDataGenerators\n",
    "    train_img_gen = ImageDataGenerator(rotation_range=10, preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "    val_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "    test_img_gen = ImageDataGenerator(preprocessing_function=remove_text,\n",
    "                                       samplewise_std_normalization=True, samplewise_center=True)\n",
    "\n",
    "    # Create DataFrameIterators\n",
    "    img_shape = tuple(cfg['DATA']['IMG_DIM'])\n",
    "    y_col = 'label_str'\n",
    "    class_mode = 'categorical'\n",
    "    train_generator = train_img_gen.flow_from_dataframe(dataframe=data['TRAIN'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False)\n",
    "    val_generator = val_img_gen.flow_from_dataframe(dataframe=data['VAL'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False)\n",
    "    test_generator = test_img_gen.flow_from_dataframe(dataframe=data['TEST'], directory=cfg['PATHS']['RAW_DATA'],\n",
    "        x_col=\"filename\", y_col=y_col, target_size=img_shape, batch_size=cfg['TRAIN']['BATCH_SIZE'],\n",
    "        class_mode=class_mode, validate_filenames=False, shuffle=False)\n",
    "\n",
    "    # Save model's ordering of class indices\n",
    "    dill.dump(test_generator.class_indices, open(cfg['PATHS']['OUTPUT_CLASS_INDICES'], 'wb'))\n",
    "\n",
    "    # Apply class imbalance strategy. We have many more X-rays negative for COVID-19 than positive.\n",
    "    histogram = np.bincount(np.array(train_generator.labels).astype(int))  # Get class distribution\n",
    "    class_weight = None\n",
    "    if cfg['TRAIN']['IMB_STRATEGY'] == 'class_weight':\n",
    "        class_multiplier = cfg['TRAIN']['CLASS_MULTIPLIER']\n",
    "        class_multiplier = [class_multiplier[cfg['DATA']['CLASSES'].index(c)] for c in test_generator.class_indices]\n",
    "        class_weight = get_class_weights(histogram, class_multiplier)\n",
    "\n",
    "    # Define metrics.\n",
    "    covid_class_idx = test_generator.class_indices['COVID-19']   # Get index of COVID-19 class\n",
    "    thresholds = 1.0 / len(cfg['DATA']['CLASSES'])      # Binary classification threshold for a class\n",
    "    metrics = [CategoricalAccuracy(name='accuracy'),\n",
    "               Precision(name='precision', thresholds=thresholds, class_id=covid_class_idx),\n",
    "               Recall(name='recall', thresholds=thresholds, class_id=covid_class_idx),\n",
    "               AUC(name='auc'),\n",
    "               F1Score(name='f1score', thresholds=thresholds, class_id=covid_class_idx)]\n",
    "\n",
    "    # Define the model.\n",
    "    print('Training distribution: ', ['Class ' + list(test_generator.class_indices.keys())[i] + ': ' + str(histogram[i]) + '. '\n",
    "           for i in range(len(histogram))])\n",
    "    input_shape = cfg['DATA']['IMG_DIM'] + [3]\n",
    "    num_gpus = cfg['TRAIN']['NUM_GPUS']\n",
    "    #debug\n",
    "    print(\"******* GPU:\", num_gpus)\n",
    "    if cfg['TRAIN']['MODEL_DEF'] == 'dcnn_resnet':\n",
    "        model_def = dcnn_resnet\n",
    "    elif cfg['TRAIN']['MODEL_DEF'] == 'resnet50v2':\n",
    "        model_def = resnet50v2\n",
    "    else:\n",
    "        model_def = resnet101v2\n",
    "    if cfg['TRAIN']['CLASS_MODE'] == 'binary':\n",
    "        histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "        output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "        model = model_def(cfg['NN']['DCNN_BINARY'], input_shape, metrics, 2, output_bias=output_bias, gpus=num_gpus)\n",
    "    else:\n",
    "        n_classes = len(cfg['DATA']['CLASSES'])\n",
    "        histogram = np.bincount(data['TRAIN']['label'].astype(int))\n",
    "        output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
    "        model = model_def(cfg['NN']['DCNN_MULTICLASS'], input_shape, metrics, n_classes, output_bias=output_bias,\n",
    "                          gpus=num_gpus)\n",
    "    #debug\n",
    "    print(\"histogram type\", type(histogram), histogram)\n",
    "        \n",
    "    # Train the model.\n",
    "    steps_per_epoch = ceil(train_generator.n / train_generator.batch_size)\n",
    "    val_steps = ceil(val_generator.n / val_generator.batch_size)\n",
    "    # debug\n",
    "    print(\"***** class weight\", class_weight)\n",
    "    class_weight = {0:26.589285714285715, 1:0.07643737166324435}\n",
    "    history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=cfg['TRAIN']['EPOCHS'],\n",
    "                                  validation_data=val_generator, validation_steps=val_steps, callbacks=callbacks,\n",
    "                                  verbose=verbose, class_weight=class_weight)\n",
    "\n",
    "    # Run the model on the test set and print the resulting performance metrics.\n",
    "    test_results = model.evaluate(test_generator, verbose=1)\n",
    "    test_metrics = {}\n",
    "    test_summary_str = [['**Metric**', '**Value**']]\n",
    "    for metric, value in zip(model.metrics_names, test_results):\n",
    "        test_metrics[metric] = value\n",
    "        print(metric, ' = ', value)\n",
    "        test_summary_str.append([metric, str(value)])\n",
    "    return model, test_metrics, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weight = {0:26.589285714285715, 1:0.07643737166324435}\n",
    "#print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_train(cfg, data, callbacks, base_log_dir):\n",
    "    '''\n",
    "    Trains a model a series of times and returns the model with the best test set metric (specified in cfg)\n",
    "    :param cfg: Project config (from config.yml)\n",
    "    :param data: Partitioned dataset\n",
    "    :param callbacks: List of callbacks to pass to model.fit()\n",
    "    :param base_log_dir: Base directory to write logs\n",
    "    :return: The trained Keras model with best test set performance on the metric specified in cfg\n",
    "    '''\n",
    "\n",
    "    # Load order of metric preference\n",
    "    metric_preference = cfg['TRAIN']['METRIC_PREFERENCE']\n",
    "    best_metrics = dict.fromkeys(metric_preference, 0.0)\n",
    "    if 'loss' in metric_preference:\n",
    "        best_metrics['loss'] = 100000.0\n",
    "\n",
    "    # Train NUM_RUNS models and return the best one according to the preferred metrics\n",
    "    for i in range(cfg['TRAIN']['NUM_RUNS']):\n",
    "        print(\"Training run \", i+1, \" / \", cfg['TRAIN']['NUM_RUNS'])\n",
    "        cur_callbacks = callbacks.copy()\n",
    "        cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "        if base_log_dir is not None:\n",
    "            log_dir = base_log_dir + cur_date\n",
    "            cur_callbacks.append(TensorBoard(log_dir=log_dir, histogram_freq=1))\n",
    "\n",
    "        # Train the model and evaluate performance on test set\n",
    "        new_model, test_metrics, test_generator = train_model(cfg, data, cur_callbacks, verbose=1)\n",
    "\n",
    "        # Log test set results and images\n",
    "        if base_log_dir is not None:\n",
    "            log_test_results(cfg, new_model, test_generator, test_metrics, log_dir)\n",
    "\n",
    "        # If this model outperforms the previous ones based on the specified metric preferences, save this one.\n",
    "        for i in range(len(metric_preference)):\n",
    "            if (((metric_preference[i] == 'loss') and (test_metrics[metric_preference[i]] < best_metrics[metric_preference[i]]))\n",
    "                    or ((metric_preference[i] != 'loss') and (test_metrics[metric_preference[i]] > best_metrics[metric_preference[i]]))):\n",
    "                best_model = new_model\n",
    "                best_metrics = test_metrics\n",
    "                best_generator = test_generator\n",
    "                best_model_date = cur_date\n",
    "                break\n",
    "            elif (test_metrics[metric_preference[i]] == best_metrics[metric_preference[i]]):\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(\"Best model test metrics: \", best_metrics)\n",
    "    return best_model, best_metrics, best_generator, best_model_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hparam_search(cfg, data, callbacks, log_dir):\n",
    "    '''\n",
    "    Conduct a random hyperparameter search over the ranges given for the hyperparameters in config.yml and log results\n",
    "    in TensorBoard. Model is trained x times for y random combinations of hyperparameters.\n",
    "    :param cfg: Project config\n",
    "    :param data: Dict containing the partitioned datasets\n",
    "    :param callbacks: List of callbacks for Keras model (excluding TensorBoard)\n",
    "    :param log_dir: Base directory in which to store logs\n",
    "    :return: (Last model trained, resultant test set metrics, test data generator)\n",
    "    '''\n",
    "\n",
    "    # Define HParam objects for each hyperparameter we wish to tune.\n",
    "    hp_ranges = cfg['HP_SEARCH']['RANGES']\n",
    "    HPARAMS = []\n",
    "    HPARAMS.append(hp.HParam('KERNEL_SIZE', hp.Discrete(hp_ranges['KERNEL_SIZE'])))\n",
    "    HPARAMS.append(hp.HParam('MAXPOOL_SIZE', hp.Discrete(hp_ranges['MAXPOOL_SIZE'])))\n",
    "    HPARAMS.append(hp.HParam('INIT_FILTERS', hp.Discrete(hp_ranges['INIT_FILTERS'])))\n",
    "    HPARAMS.append(hp.HParam('FILTER_EXP_BASE', hp.IntInterval(hp_ranges['FILTER_EXP_BASE'][0], hp_ranges['FILTER_EXP_BASE'][1])))\n",
    "    HPARAMS.append(hp.HParam('NODES_DENSE0', hp.Discrete(hp_ranges['NODES_DENSE0'])))\n",
    "    HPARAMS.append(hp.HParam('CONV_BLOCKS', hp.IntInterval(hp_ranges['CONV_BLOCKS'][0], hp_ranges['CONV_BLOCKS'][1])))\n",
    "    HPARAMS.append(hp.HParam('DROPOUT', hp.Discrete(hp_ranges['DROPOUT'])))\n",
    "    HPARAMS.append(hp.HParam('LR', hp.RealInterval(hp_ranges['LR'][0], hp_ranges['LR'][1])))\n",
    "    HPARAMS.append(hp.HParam('OPTIMIZER', hp.Discrete(hp_ranges['OPTIMIZER'])))\n",
    "    HPARAMS.append(hp.HParam('L2_LAMBDA', hp.Discrete(hp_ranges['L2_LAMBDA'])))\n",
    "    HPARAMS.append(hp.HParam('BATCH_SIZE', hp.Discrete(hp_ranges['BATCH_SIZE'])))\n",
    "    HPARAMS.append(hp.HParam('IMB_STRATEGY', hp.Discrete(hp_ranges['IMB_STRATEGY'])))\n",
    "\n",
    "    # Define test set metrics that we wish to log to TensorBoard for each training run\n",
    "    HP_METRICS = [hp.Metric(metric, display_name='Test ' + metric) for metric in cfg['HP_SEARCH']['METRICS']]\n",
    "\n",
    "    # Configure TensorBoard to log the results\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams_config(hparams=HPARAMS, metrics=HP_METRICS)\n",
    "\n",
    "    # Complete a number of training runs at different hparam values and log the results.\n",
    "    repeats_per_combo = cfg['HP_SEARCH']['REPEATS']   # Number of times to train the model per combination of hparams\n",
    "    num_combos = cfg['HP_SEARCH']['COMBINATIONS']     # Number of random combinations of hparams to attempt\n",
    "    num_sessions = num_combos * repeats_per_combo       # Total number of runs in this experiment\n",
    "    model_type = 'DCNN_BINARY' if cfg['TRAIN']['CLASS_MODE'] == 'binary' else 'DCNN_MULTICLASS'\n",
    "    trial_id = 0\n",
    "    for group_idx in range(num_combos):\n",
    "        rand = random.Random()\n",
    "        HPARAMS = {h: h.domain.sample_uniform(rand) for h in HPARAMS}\n",
    "        hparams = {h.name: HPARAMS[h] for h in HPARAMS}  # To pass to model definition\n",
    "        for repeat_idx in range(repeats_per_combo):\n",
    "            trial_id += 1\n",
    "            print(\"Running training session %d/%d\" % (trial_id, num_sessions))\n",
    "            print(\"Hparam values: \", {h.name: HPARAMS[h] for h in HPARAMS})\n",
    "            trial_logdir = os.path.join(log_dir, str(trial_id))     # Need specific logdir for each trial\n",
    "            callbacks_hp = callbacks + [TensorBoard(log_dir=trial_logdir, profile_batch=0, write_graph=False)]\n",
    "\n",
    "            # Set values of hyperparameters for this run in config file.\n",
    "            for h in hparams:\n",
    "                if h in ['LR', 'L2_LAMBDA']:\n",
    "                    val = 10 ** hparams[h]      # These hyperparameters are sampled on the log scale.\n",
    "                else:\n",
    "                    val = hparams[h]\n",
    "                cfg['NN'][model_type][h] = val\n",
    "\n",
    "            # Set some hyperparameters that are not specified in model definition.\n",
    "            cfg['TRAIN']['BATCH_SIZE'] = hparams['BATCH_SIZE']\n",
    "            cfg['TRAIN']['IMB_STRATEGY'] = hparams['IMB_STRATEGY']\n",
    "\n",
    "            # Run a training session and log the performance metrics on the test set to HParams dashboard in TensorBoard\n",
    "            with tf.summary.create_file_writer(trial_logdir).as_default():\n",
    "                hp.hparams(HPARAMS, trial_id=str(trial_id))\n",
    "                model, test_metrics, test_generator = train_model(cfg, data, callbacks_hp, verbose=0)\n",
    "                for metric in HP_METRICS:\n",
    "                    if metric._tag in test_metrics:\n",
    "                        tf.summary.scalar(metric._tag, test_metrics[metric._tag], step=1)   # Log test metric\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_test_results(cfg, model, test_generator, test_metrics, log_dir):\n",
    "    '''\n",
    "    Visualize performance of a trained model on the test set. Optionally save the model.\n",
    "    :param cfg: Project config\n",
    "    :param model: A trained Keras model\n",
    "    :param test_generator: A Keras generator for the test set\n",
    "    :param test_metrics: Dict of test set performance metrics\n",
    "    :param log_dir: Path to write TensorBoard logs\n",
    "    '''\n",
    "\n",
    "    # Visualization of test results\n",
    "    test_predictions = model.predict(test_generator, verbose=0)\n",
    "    test_labels = test_generator.labels\n",
    "    covid_idx = test_generator.class_indices['COVID-19']\n",
    "    plt = plot_roc(\"Test set\", test_labels, test_predictions, class_id=covid_idx)\n",
    "    roc_img = plot_to_tensor()\n",
    "    plt = plot_confusion_matrix(test_labels, test_predictions, class_id=covid_idx)\n",
    "    cm_img = plot_to_tensor()\n",
    "\n",
    "    # Log test set results and plots in TensorBoard\n",
    "    writer = tf_summary.create_file_writer(logdir=log_dir)\n",
    "\n",
    "    # Create table of test set metrics\n",
    "    test_summary_str = [['**Metric**','**Value**']]\n",
    "    thresholds = cfg['TRAIN']['THRESHOLDS']  # Load classification thresholds\n",
    "    for metric in test_metrics:\n",
    "        if metric in ['precision', 'recall'] and isinstance(metric, list):\n",
    "            metric_values = dict(zip(thresholds, test_metrics[metric]))\n",
    "        else:\n",
    "            metric_values = test_metrics[metric]\n",
    "        test_summary_str.append([metric, str(metric_values)])\n",
    "\n",
    "    # Create table of model and train config values\n",
    "    hparam_summary_str = [['**Variable**', '**Value**']]\n",
    "    for key in cfg['TRAIN']:\n",
    "        hparam_summary_str.append([key, str(cfg['TRAIN'][key])])\n",
    "    if cfg['TRAIN']['CLASS_MODE'] == 'binary':\n",
    "        for key in cfg['NN']['DCNN_BINARY']:\n",
    "            hparam_summary_str.append([key, str(cfg['NN']['DCNN_BINARY'][key])])\n",
    "    else:\n",
    "        for key in cfg['NN']['DCNN_BINARY']:\n",
    "            hparam_summary_str.append([key, str(cfg['NN']['DCNN_BINARY'][key])])\n",
    "\n",
    "    # Write to TensorBoard logs\n",
    "    with writer.as_default():\n",
    "        tf_summary.text(name='Test set metrics', data=tf.convert_to_tensor(test_summary_str), step=0)\n",
    "        tf_summary.text(name='Run hyperparameters', data=tf.convert_to_tensor(hparam_summary_str), step=0)\n",
    "        tf_summary.image(name='ROC Curve (Test Set)', data=roc_img, step=0)\n",
    "        tf_summary.image(name='Confusion Matrix (Test Set)', data=cm_img, step=0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experiment(cfg=None, experiment='single_train', save_weights=True, write_logs=True):\n",
    "    '''\n",
    "    Defines and trains HIFIS-v2 model. Prints and logs relevant metrics.\n",
    "    :param experiment: The type of training experiment. Choices are {'single_train'}\n",
    "    :param save_weights: A flag indicating whether to save the model weights\n",
    "    :param write_logs: A flag indicating whether to write TensorBoard logs\n",
    "    :return: A dictionary of metrics on the test set\n",
    "    '''\n",
    "\n",
    "    # Load project config data\n",
    "    if cfg is None:\n",
    "        cfg = yaml.full_load(open(os.getcwd() + \"/config.yml\", 'r'))\n",
    "\n",
    "    # Set logs directory\n",
    "    cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    print(cfg['PATHS']['LOGS'])\n",
    "    log_dir = cfg['PATHS']['LOGS'] + \"training/\" + cur_date if write_logs else None\n",
    "    if not os.path.exists(cfg['PATHS']['LOGS'] + \"training\\\\\"):\n",
    "        os.makedirs(cfg['PATHS']['LOGS'] + \"training\\\\\")\n",
    "\n",
    "    # Load dataset file paths and labels\n",
    "    data = {}\n",
    "    data['TRAIN'] = pd.read_csv(cfg['PATHS']['TRAIN_SET'])\n",
    "    data['VAL'] = pd.read_csv(cfg['PATHS']['VAL_SET'])\n",
    "    data['TEST'] = pd.read_csv(cfg['PATHS']['TEST_SET'])\n",
    "\n",
    "    # Set callbacks.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=cfg['TRAIN']['PATIENCE'], mode='min', restore_best_weights=True)\n",
    "    callbacks = [early_stopping]\n",
    "\n",
    "    # Conduct the desired train experiment\n",
    "    if experiment == 'hparam_search':\n",
    "        log_dir = cfg['PATHS']['LOGS'] + \"hparam_search\\\\\" + cur_date\n",
    "        random_hparam_search(cfg, data, callbacks, log_dir)\n",
    "    else:\n",
    "        if experiment == 'multi_train':\n",
    "            base_log_dir = cfg['PATHS']['LOGS'] + \"training\\\\\" if write_logs else None\n",
    "            model, test_metrics, test_generator, cur_date = multi_train(cfg, data, callbacks, base_log_dir)\n",
    "        else:\n",
    "            if write_logs:\n",
    "                tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                callbacks.append(tensorboard)\n",
    "            #\n",
    "            '''\n",
    "            print(cfg)\n",
    "            print(data)\n",
    "            print(callbacks)\n",
    "            '''\n",
    "            model, test_metrics, test_generator = train_model(cfg, data, callbacks)\n",
    "            print(\"single\", model)\n",
    "            if write_logs:\n",
    "                log_test_results(cfg, model, test_generator, test_metrics, log_dir)\n",
    "        if save_weights:\n",
    "            model_path = cfg['PATHS']['MODEL_WEIGHTS'] + 'model' + cur_date + '.h5'\n",
    "            save_model(model, model_path)  # Save the model's weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'PATHS': {'RAW_DATA': '/home/ubuntu/covid-cxr/data/', 'MILA_DATA': '/home/ubuntu/covid-cxr/data/covid-chestxray-dataset/', 'FIGURE1_DATA': '/home/ubuntu/covid-cxr/data/Figure1-COVID-chestxray-dataset/', 'RSNA_DATA': '/home/ubuntu/covid-cxr/data/rsna/', 'PROCESSED_DATA': 'data/processed/', 'TRAIN_SET': '/home/ubuntu/covid-cxr/data/processed/train_set.csv', 'VAL_SET': '/home/ubuntu/covid-cxr/data/processed/val_set.csv', 'TEST_SET': '/home/ubuntu/covid-cxr/data/processed/test_set.csv', 'IMAGES': '/home/ubuntu/covid-cxr/documents/generated_images/', 'LOGS': '/home/ubuntu/covid-cxr/results/logs/', 'MODEL_WEIGHTS': '/home/ubuntu/covid-cxr/results/models/', 'MODEL_TO_LOAD': '/home/ubuntu/covid-cxr/results/models/model20201102-033225.h5', 'LIME_EXPLAINER': '/home/ubuntu/covid-cxr/data/interpretability/lime_explainer.pkl', 'OUTPUT_CLASS_INDICES': '/home/ubuntu/covid-cxr/data/interpretability/output_class_indices.pkl', 'BATCH_PRED_IMGS': '/home/ubuntu/covid-cxr/data/processed/test/', 'BATCH_PREDS': '/home/ubuntu/covid-cxr/results/predictions/'}, 'DATA': {'IMG_DIM': [224, 224], 'VIEWS': ['PA', 'AP'], 'VAL_SPLIT': 0.08, 'TEST_SPLIT': 0.1, 'NUM_RSNA_IMGS': 1000, 'CLASSES': ['non-COVID-19', 'COVID-19']}, 'TRAIN': {'CLASS_MODE': 'binary', 'MODEL_DEF': 'dcnn_resnet', 'CLASS_MULTIPLIER': [0.15, 1.0], 'EXPERIMENT_TYPE': 'single_train', 'BATCH_SIZE': 32, 'EPOCHS': 1, 'THRESHOLDS': 0.5, 'PATIENCE': 7, 'IMB_STRATEGY': 'class_weight', 'METRIC_PREFERENCE': ['auc', 'recall', 'precision', 'loss'], 'NUM_RUNS': 10, 'NUM_GPUS': 1}, 'NN': {'DCNN_BINARY': {'KERNEL_SIZE': '(3,3)', 'STRIDES': '(1,1)', 'INIT_FILTERS': 16, 'FILTER_EXP_BASE': 3, 'MAXPOOL_SIZE': '(2,2)', 'CONV_BLOCKS': 3, 'NODES_DENSE0': 128, 'LR': 1e-05, 'OPTIMIZER': 'adam', 'DROPOUT': 0.4, 'L2_LAMBDA': 0.0001}, 'DCNN_MULTICLASS': {'KERNEL_SIZE': '(3,3)', 'STRIDES': '(1,1)', 'INIT_FILTERS': 16, 'FILTER_EXP_BASE': 3, 'MAXPOOL_SIZE': '(2,2)', 'CONV_BLOCKS': 4, 'NODES_DENSE0': 128, 'LR': 0.0002, 'OPTIMIZER': 'adam', 'DROPOUT': 0.4, 'L2_LAMBDA': 0.0001}}, 'LIME': {'KERNEL_WIDTH': 1.75, 'FEATURE_SELECTION': 'lasso_path', 'NUM_FEATURES': 1000, 'NUM_SAMPLES': 1000, 'COVID_ONLY': False}, 'HP_SEARCH': {'METRICS': ['accuracy', 'loss', 'recall', 'precision', 'auc'], 'COMBINATIONS': 50, 'REPEATS': 2, 'RANGES': {'KERNEL_SIZE': ['(3,3)', '(5,5)'], 'MAXPOOL_SIZE': ['(2,2)', '(3,3)'], 'INIT_FILTERS': [8, 16, 32], 'FILTER_EXP_BASE': [2, 3], 'NODES_DENSE0': [128, 256, 512, 1024], 'CONV_BLOCKS': [3, 8], 'DROPOUT': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], 'LR': [-5.0, -3.0], 'OPTIMIZER': ['adam'], 'L2_LAMBDA': [0.0, 1e-05, 0.0001, 0.001], 'BATCH_SIZE': [16, 32], 'IMB_STRATEGY': ['class_weight']}}, 'PREDICTION': {'THRESHOLD': 0.5}}\n",
      "single_train\n",
      "/home/ubuntu/covid-cxr/results/logs/\n",
      "Found 1489 non-validated image filenames belonging to 2 classes.\n",
      "Found 146 non-validated image filenames belonging to 2 classes.\n",
      "Found 182 non-validated image filenames belonging to 2 classes.\n",
      "Class weights:  [26.589285714285715, 0.07643737166324435]\n",
      "Class weights type: <class 'list'>\n",
      "Training distribution:  ['Class COVID-19: 28. ', 'Class non-COVID-19: 1461. ']\n",
      "******* GPU: 1\n",
      "MODEL CONFIG:  {'KERNEL_SIZE': '(3,3)', 'STRIDES': '(1,1)', 'INIT_FILTERS': 16, 'FILTER_EXP_BASE': 3, 'MAXPOOL_SIZE': '(2,2)', 'CONV_BLOCKS': 3, 'NODES_DENSE0': 128, 'LR': 1e-05, 'OPTIMIZER': 'adam', 'DROPOUT': 0.4, 'L2_LAMBDA': 0.0001}\n",
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_0 (Conv2D)                (None, 224, 224, 16) 448         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 224, 224, 16) 64          conv0_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 224, 224, 16) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv0_1 (Conv2D)                (None, 224, 224, 16) 2320        leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat0 (Concatenate)           (None, 224, 224, 19) 0           conv0_1[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 224, 224, 19) 76          concat0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 224, 224, 19) 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 112, 112, 19) 0           leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_0 (Conv2D)                (None, 112, 112, 48) 8256        max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 112, 112, 48) 192         conv1_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 112, 112, 48) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 112, 112, 48) 20784       leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat1 (Concatenate)           (None, 112, 112, 67) 0           conv1_1[0][0]                    \n",
      "                                                                 max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 112, 112, 67) 268         concat1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 112, 112, 67) 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 56, 56, 67)   0           leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_0 (Conv2D)                (None, 56, 56, 144)  86976       max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 56, 56, 144)  576         conv2_0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 56, 56, 144)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 56, 56, 144)  186768      leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concat2 (Concatenate)           (None, 56, 56, 211)  0           conv2_1[0][0]                    \n",
      "                                                                 max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 56, 56, 211)  844         concat2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 56, 56, 211)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 28, 28, 211)  0           leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 165424)       0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 165424)       0           flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          21174400    dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 128)          0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            258         leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output (Activation)             (None, 2)            0           dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,482,230\n",
      "Trainable params: 21,481,220\n",
      "Non-trainable params: 1,010\n",
      "__________________________________________________________________________________________________\n",
      "histogram type <class 'numpy.ndarray'> [1461   28]\n",
      "***** class weight [26.589285714285715, 0.07643737166324435]\n",
      "47/47 [==============================] - 53s 1s/step - batch: 23.0000 - size: 31.6809 - loss: 537.1755 - accuracy: 0.2216 - precision: 0.0163 - recall: 0.6786 - auc: 0.1498 - f1score: 0.0317 - val_loss: 465.3458 - val_accuracy: 0.0274 - val_precision: 0.0207 - val_recall: 1.0000 - val_auc: 0.0285 - val_f1score: 0.0405\n",
      "loss  =  466.6340789794922\n",
      "accuracy  =  0.06593407\n",
      "precision  =  0.022988506\n",
      "recall  =  1.0\n",
      "auc  =  0.036650166\n",
      "f1score  =  0.044943817\n",
      "True (-)ves:  8 \n",
      "False (+)ves:  170 \n",
      "False (-)ves:  0 \n",
      "True (+)ves:  4\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'ROCCurveTestSet_6/write_summary/assert_non_negative/assert_less_equal/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py\", line 947, in assert_less_equal\n",
      "    np.less_equal, x, y, data, summarize, message, name)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py\", line 373, in _binary_assert\n",
      "    return control_flow_ops.Assert(condition, data, summarize=summarize)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.framework.ops.Operation'>):\n",
      "<tf.Operation 'ConfusionMatrixTestSet_6/write_summary/assert_non_negative/assert_less_equal/Assert/Assert' type=Assert>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py\", line 947, in assert_less_equal\n",
      "    np.less_equal, x, y, data, summarize, message, name)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/ops/check_ops.py\", line 373, in _binary_assert\n",
      "    return control_flow_ops.Assert(condition, data, summarize=summarize)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\n",
      "    return target(*args, **kwargs)  File \"/opt/tljh/user/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
      "    error_in_function=error_in_function)\n",
      "==================================\n",
      "This is model:  None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#epoch comes from config.yml\n",
    "cfg = yaml.full_load(open(\"/home/ubuntu/covid-cxr/config.yml\", 'r'))\n",
    "print(\"1\",cfg)\n",
    "print(cfg['TRAIN']['EXPERIMENT_TYPE']) #single train\n",
    "\n",
    "model = train_experiment(cfg=cfg, experiment=cfg['TRAIN']['EXPERIMENT_TYPE'], save_weights=True, write_logs=True)\n",
    "print(\"This is model: \",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snap\n",
    "\n",
    "to_explain = X[[39,41,45]]\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "\n",
    "layer = 9\n",
    "\n",
    "e = shap.GradientExplainer((model.layers[layer].input, model.layers[-1].output), map2layer(preprocess_input(X.copy()), layer))\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, layer), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
