# Section II. Summary Paper

## Evolution of Machine Learning

Recent advances in AI and ML have propelled the need to explain the models predictions. As more industries adopting AI/ML for all kinds of applications like forecasting sales, ranking products, rating employees, etc. We need better understanding how these complex AI/ML models behave when it achieves positive predictions as well as failed predictions.

## Who Needs to Know

Data Scientists – These individuals who works and creates ML models. They are the first hands to experience and to reveal the underlining behaviors of the models. Having an explainable model would add value during ML development and troubleshooting. Especially valuable with image type of classifications to aid identify features contributed to predictions.

Domain Experts – These are the individual with specialization in a particular field. Such as doctors that looks at a patient’s x-ray to decide if the case of ailment is positive or negative. Their extensive training and experience would complement the ML models. It is especially critical that when there is a disagreement between the model and human judgement. We need to know how the model made the conclusion and factors it used for the decision.

Business Stakeholders – These individuals would make the decision if the ML program would have the necessary funding. The ML model must provide explanations that are understandable in their terminologies. Buy-ins from the stakeholders is critical to proliferation the ML program.

End Users – These are the people that will use the ML program on a daily basis. They are the eyes and guards of the model’s behavior. The human intuition coupled with explainable ML program would build trust and provide additional benefits in decision making.

## What Explainable Models are in Use:

